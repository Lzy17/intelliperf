L2 Hit Rate Improvement %: 4.562052483651357
Speedup: 1.0264466417128555

Swizzling Pattern:
[[[xcd_id = pid % num_XCD
local_block_id = pid // num_XCD
new_pid = (local_block_id + xcd_id * (num_blocks // num_XCD)) % num_blocks]]]
Full Kernel Code:
[[[#!/usr/bin/env python

import torch
import triton
import triton.language as tl
import argparse

@triton.jit
def conv2d_kernel(
    x_ptr, w_ptr, y_ptr,
    N, C, H, W,
    F, R, S,
    stride_xn, stride_xc, stride_xh, stride_xw,
    stride_wn, stride_wc, stride_wh, stride_ww,
    stride_yn, stride_yc, stride_yh, stride_yw,
    P_H, P_W,
    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_F: tl.constexpr,
    BLOCK_SIZE_H: tl.constexpr, BLOCK_SIZE_W: tl.constexpr,
    BLOCK_SIZE_C: tl.constexpr
):
    num_XCD = 8
    num_blocks = tl.cdiv(N, BLOCK_SIZE_N) * tl.cdiv(F, BLOCK_SIZE_F) * tl.cdiv(P_H, BLOCK_SIZE_H) * tl.cdiv(P_W, BLOCK_SIZE_W)
    pid = tl.program_id(axis=0)
    
    # New swizzling pattern
    xcd_id = pid % num_XCD
    local_block_id = pid // num_XCD
    new_pid = (local_block_id + xcd_id * (num_blocks // num_XCD)) % num_blocks

    num_pid_w = tl.cdiv(P_W, BLOCK_SIZE_W)
    num_pid_h = tl.cdiv(P_H, BLOCK_SIZE_H)
    num_pid_f = tl.cdiv(F, BLOCK_SIZE_F)
    
    pid_w = new_pid % num_pid_w
    pid_h = (new_pid // num_pid_w) % num_pid_h
    pid_f = (new_pid // (num_pid_w * num_pid_h)) % num_pid_f
    pid_n = new_pid // (num_pid_w * num_pid_h * num_pid_f)

    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    offs_f = pid_f * BLOCK_SIZE_F + tl.arange(0, BLOCK_SIZE_F)
    offs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)
    offs_w = pid_w * BLOCK_SIZE_W + tl.arange(0, BLOCK_SIZE_W)

    offs_y_n = offs_n[:, None, None, None]
    offs_y_f = offs_f[None, :, None, None]
    offs_y_h = offs_h[None, None, :, None]
    offs_y_w = offs_w[None, None, None, :]

    y_ptrs = y_ptr + offs_y_n * stride_yn + offs_y_f * stride_yc + \
             offs_y_h * stride_yh + offs_y_w * stride_yw

    accumulator = tl.zeros((BLOCK_SIZE_N, BLOCK_SIZE_F, BLOCK_SIZE_H, BLOCK_SIZE_W), dtype=tl.float32)

    offs_c = tl.arange(0, BLOCK_SIZE_C)
    
    for r in range(R):
        for s in range(S):
            offs_x_n = offs_n[:, None, None, None]
            offs_x_c = offs_c[None, :, None, None]
            offs_x_h = offs_h[None, None, :, None] + r
            offs_x_w = offs_w[None, None, None, :] + s
            
            x_ptrs = x_ptr + offs_x_n * stride_xn + offs_x_c * stride_xc + offs_x_h * stride_xh + offs_x_w * stride_xw
            mask_x = (offs_x_n < N) & (offs_x_c < C) & (offs_x_h < H) & (offs_x_w < W)
            x = tl.load(x_ptrs, mask=mask_x, other=0.0)

            offs_w_f = offs_f[None, :, None, None]
            offs_w_c = offs_c[None, :, None, None]
            
            w_ptrs = w_ptr + offs_w_f * stride_wn + offs_w_c * stride_wc + r * stride_wh + s * stride_ww
            mask_w = (offs_w_f < F) & (offs_w_c < C) 
            w = tl.load(w_ptrs, mask=mask_w, other=0.0)
            
            accumulator += tl.sum(x * w, axis=1)

    y = accumulator.to(tl.float16)
    mask_y = (offs_y_n < N) & (offs_y_f < F) & (offs_y_h < P_H) & (offs_y_w < P_W)
    tl.store(y_ptrs, y, mask=mask_y)


def conv2d(x, w):
    N, C, H, W = x.shape
    F, _, R, S = w.shape
    P_H, P_W = H - R + 1, W - S + 1 
    y = torch.empty((N, F, P_H, P_W), device=x.device, dtype=x.dtype)

    grid = lambda META: (
        triton.cdiv(N, META['BLOCK_SIZE_N']) * 
        triton.cdiv(F, META['BLOCK_SIZE_F']) *
        triton.cdiv(P_H, META['BLOCK_SIZE_H']) *
        triton.cdiv(P_W, META['BLOCK_SIZE_W']),
    )

    conv2d_kernel[grid](
        x, w, y,
        N, C, H, W,
        F, R, S,
        x.stride(0), x.stride(1), x.stride(2), x.stride(3),
        w.stride(0), w.stride(1), w.stride(2), w.stride(3),
        y.stride(0), y.stride(1), y.stride(2), y.stride(3),
        P_H, P_W,
        BLOCK_SIZE_N=16, BLOCK_SIZE_F=16,
        BLOCK_SIZE_H=16, BLOCK_SIZE_W=16,
        BLOCK_SIZE_C=16
    )
    return y

def main(N=32, C=32, H=32, W=32, F=64, R=3, S=3):
    x = torch.randn((N, C, H, W), device='cuda', dtype=torch.float16)
    w = torch.randn((F, C, R, S), device='cuda', dtype=torch.float16)
    
    rep = 100
    
    for _ in range(10):
        y_triton = conv2d(x, w)

    torch.cuda.synchronize()
    start_time = torch.cuda.Event(enable_timing=True)
    end_time = torch.cuda.Event(enable_timing=True)

    start_time.record()
    for _ in range(rep):
        y_triton = conv2d(x, w)
    end_time.record()
    torch.cuda.synchronize()

    triton_time = start_time.elapsed_time(end_time) / rep
    print(f"Triton conv2d time: {triton_time:.4f} ms")

    # y_torch = torch.nn.functional.conv2d(x, w)
    # print(f"Triton output: {y_triton}")
    # print(f"Torch output: {y_torch}")
    # assert torch.allclose(y_triton, y_torch, atol=1e-1, rtol=0), "Triton and PyTorch results differ"


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Triton Conv2D Benchmark")
    parser.add_argument("--N", type=int, default=32, help="Batch size")
    parser.add_argument("--C", type=int, default=32, help="Input channels")
    parser.add_argument("--H", type=int, default=32, help="Input height")
    parser.add_argument("--W", type=int, default=32, help="Input width")
    parser.add_argument("--F", type=int, default=64, help="Output channels")
    parser.add_argument("--R", type=int, default=3, help="Filter height")
    parser.add_argument("--S", type=int, default=3, help="Filter width")
    args = parser.parse_args()
    
    main(args.N, args.C, args.H, args.W, args.F, args.R, args.S)]]]
